# news_sentiment_analyzer.py - å°è‚¡æ–°èæƒ…ç·’åˆ†æå™¨

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
from dataclasses import dataclass
import jieba
from collections import Counter
import urllib.parse

@dataclass
class NewsArticle:
    """æ–°èæ–‡ç« è³‡æ–™çµæ§‹"""
    title: str
    content: str
    url: str
    publish_time: str
    source: str

@dataclass
class SentimentAnalysis:
    """æƒ…ç·’åˆ†æçµæœ"""
    sentiment: str  # 'POSITIVE', 'NEGATIVE', 'NEUTRAL'
    confidence: float  # ä¿¡å¿ƒåº¦ 0-100
    positive_score: int
    negative_score: int
    key_positive_words: List[str]
    key_negative_words: List[str]
    summary: str

class NewsEmotionAnalyzer:
    """æ–°èæƒ…ç·’åˆ†æå™¨"""
    
    def __init__(self):
        # å°è‚¡å°ˆç”¨æƒ…ç·’å­—å…¸
        self.positive_keywords = {
            # æ¥­ç¸¾ç›¸é—œ
            'ç²åˆ©': 3, 'ç‡Ÿæ”¶': 2, 'æˆé•·': 3, 'å¢é•·': 3, 'çªç ´': 3, 'å‰µæ–°é«˜': 4, 'æ–°é«˜': 3,
            'è¶…è¶Š': 3, 'å„ªæ–¼': 2, 'å‹é': 2, 'é ˜å…ˆ': 3, 'é¾é ­': 3, 'ç¬¬ä¸€': 2,
            
            # æŠ€è¡“é¢ç›¸é—œ  
            'ä¸Šæ¼²': 2, 'æ¼²å¹…': 2, 'å¼·å‹¢': 3, 'çªç ´': 3, 'æ”»å…‹': 3, 'ç«™ä¸Š': 2, 'æ”¶å¾©': 2,
            'åå½ˆ': 2, 'å›å‡': 2, 'èµ°å¼·': 2, 'è½‰å¼·': 3, 'å¤šé ­': 3, 'è²·ç›¤': 2,
            
            # åŸºæœ¬é¢ç›¸é—œ
            'åˆ©å¤š': 3, 'çœ‹å¥½': 3, 'æ¨‚è§€': 3, 'æ­£é¢': 2, 'ç©æ¥µ': 2, 'æœ‰åˆ©': 2,
            'å—æƒ ': 2, 'åŠ©ç›Š': 2, 'æ¨å‡': 2, 'å¸¶å‹•': 2, 'æ¿€å‹µ': 3, 'åˆºæ¿€': 2,
            
            # å…¬å¸ç‡Ÿé‹
            'æ“´å» ': 2, 'ä½µè³¼': 2, 'åˆä½œ': 2, 'ç°½ç´„': 2, 'å¾—æ¨™': 3, 'è¨‚å–®': 2,
            'æŠ•è³‡': 1, 'å¸ƒå±€': 2, 'é€²è»': 2, 'æ‹“å±•': 2, 'é–‹ç™¼': 2, 'ç ”ç™¼': 2,
            
            # å¸‚å ´è©•åƒ¹
            'çœ‹æ¼²': 3, 'æ¨è–¦': 2, 'è²·é€²': 3, 'åŠ ç¢¼': 3, 'èª¿å‡': 3, 'ä¸Šä¿®': 3,
            'ç›®æ¨™åƒ¹': 2, 'æ½›åŠ›': 2, 'æ©Ÿæœƒ': 2, 'è½‰æ©Ÿ': 3, 'é¡Œæ': 2, 'æ¦‚å¿µ': 1,
            
            # ä¸€èˆ¬æ­£é¢è©å½™
            'å¥½': 1, 'ä½³': 2, 'å„ª': 2, 'å¼·': 2, 'é«˜': 1, 'å¤§': 1, 'å¤š': 1,
            'å‡': 2, 'æ¼²': 2, 'å¢': 2, 'è³º': 3, 'ç›ˆ': 2, 'è±': 2
        }
        
        self.negative_keywords = {
            # æ¥­ç¸¾ç›¸é—œ
            'è™§æ': 4, 'è¡°é€€': 3, 'ä¸‹æ»‘': 3, 'æ¸›å°‘': 2, 'ç¸®æ°´': 3, 'æ…˜æ·¡': 4,
            'ä½è¿·': 3, 'ç–²å¼±': 3, 'ä¸æŒ¯': 3, 'è¡¨ç¾å·®': 3, 'å¤±æœ›': 3,
            
            # æŠ€è¡“é¢ç›¸é—œ
            'ä¸‹è·Œ': 2, 'è·Œå¹…': 2, 'å¼±å‹¢': 3, 'è·Œç ´': 3, 'å¤±å®ˆ': 3, 'é‡æŒ«': 4,
            'å¤§è·Œ': 4, 'æš´è·Œ': 4, 'å´©è·Œ': 5, 'æ®ºç›¤': 4, 'è³£å£“': 3, 'ç©ºé ­': 3,
            
            # é¢¨éšªç›¸é—œ
            'åˆ©ç©º': 3, 'çœ‹å£': 3, 'æ‚²è§€': 3, 'è² é¢': 2, 'æ“”æ†‚': 2, 'æ†‚æ…®': 2,
            'è­¦å‘Š': 3, 'å±æ©Ÿ': 4, 'é¢¨éšª': 2, 'å¨è„…': 3, 'è¡æ“Š': 3, 'å£“åŠ›': 2,
            
            # å¸‚å ´è©•åƒ¹
            'çœ‹è·Œ': 3, 'è³£å‡º': 3, 'æ¸›ç¢¼': 3, 'èª¿é™': 3, 'ä¸‹ä¿®': 3, 'é™è©•': 3,
            'ä¸æ¨è–¦': 3, 'é¿é–‹': 3, 'å‡ºå ´': 3, 'å¥—ç‰¢': 3, 'è¢«å¥—': 3,
            
            # ç‡Ÿé‹å•é¡Œ
            'åœç”¢': 4, 'é—œå» ': 4, 'è£å“¡': 3, 'å€’é–‰': 5, 'é€€å‡º': 3, 'çµ‚æ­¢': 3,
            'å»¶å¾Œ': 2, 'æš«åœ': 2, 'å–æ¶ˆ': 3, 'å¤±åˆ©': 3, 'å¤±æ•—': 3,
            
            # ä¸€èˆ¬è² é¢è©å½™
            'å·®': 2, 'åŠ£': 3, 'å¼±': 2, 'ä½': 1, 'å°‘': 1, 'å°': 1,
            'è·Œ': 2, 'é™': 2, 'æ¸›': 2, 'è™§': 3, 'æ': 3, 'æ…˜': 4
        }
        
        # ä¸­æ€§ä½†é‡è¦çš„é—œéµå­—ï¼ˆç”¨æ–¼è­˜åˆ¥é‡è¦è­°é¡Œï¼‰
        self.important_keywords = {
            'è²¡å ±', 'æ³•èªªæœƒ', 'è‘£äº‹æœƒ', 'è‚¡æ±æœƒ', 'é™¤æ¬Š', 'é™¤æ¯', 'è‚¡åˆ©', 'é…æ¯',
            'åˆä½µ', 'åˆ†å‰²', 'é‡çµ„', 'è½‰æŠ•è³‡', 'å­å…¬å¸', 'é—œä¿‚ä¼æ¥­',
            'ç”¢èƒ½', 'ç¨¼å‹•ç‡', 'æ¯›åˆ©ç‡', 'ç‡Ÿæ¥­åˆ©ç›Š', 'ç¨…å¾Œæ·¨åˆ©', 'EPS',
            'ç¾é‡‘æµ', 'è² å‚µæ¯”', 'ROE', 'ROA', 'æœ¬ç›Šæ¯”', 'è‚¡åƒ¹æ·¨å€¼æ¯”',
            'å¸‚å ´å æœ‰ç‡', 'ç«¶çˆ­å°æ‰‹', 'ç”¢æ¥­éˆ', 'ä¾›æ‡‰å•†', 'å®¢æˆ¶',
            'æ³•è¦', 'æ”¿ç­–', 'è£œåŠ©', 'é—œç¨…', 'åŒ¯ç‡', 'åˆ©ç‡', 'é€šè†¨'
        }
        
        # è¨­ç½® jieba åˆ†è©
        jieba.initialize()
        
    def crawl_cnyes_news(self, stock_code: str, stock_name: str, max_articles: int = 10) -> List[NewsArticle]:
        """çˆ¬å–é‰…äº¨ç¶²æ–°è - å¢å¼·ç‰ˆ"""
        print(f"æ­£åœ¨çˆ¬å–é‰…äº¨ç¶²é—œæ–¼ {stock_code} {stock_name} çš„æ–°è...")
        articles = []
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
        }
        
        # å˜—è©¦å¤šå€‹æœå°‹ç­–ç•¥
        search_urls = [
            f"https://news.cnyes.com/news/cat/tw_stock?keyword={urllib.parse.quote(stock_code)}",
            f"https://news.cnyes.com/news/cat/tw_stock?keyword={urllib.parse.quote(stock_name)}",
            f"https://news.cnyes.com/news/cat/tw_stock",  # ä¸€èˆ¬å°è‚¡æ–°è
        ]
        
        for search_url in search_urls:
            try:
                print(f"å˜—è©¦URL: {search_url}")
                response = requests.get(search_url, headers=headers, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # å¤šç¨®é¸æ“‡å™¨ç­–ç•¥
                selectors = [
                    'a[href*="/news/id/"]',
                    '.news-list a',
                    '.title a',
                    'h3 a',
                    'h2 a',
                    '.item-title a'
                ]
                
                news_links = []
                for selector in selectors:
                    found_links = soup.select(selector)
                    if found_links:
                        news_links.extend(found_links)
                        break
                
                print(f"æ‰¾åˆ° {len(news_links)} å€‹æ–°èé€£çµ")
                
                for link in news_links:
                    if len(articles) >= max_articles:
                        break
                        
                    try:
                        news_url = link.get('href', '')
                        if not news_url:
                            continue
                            
                        if not news_url.startswith('http'):
                            news_url = 'https://news.cnyes.com' + news_url
                        
                        news_title = link.get_text(strip=True)
                        if not news_title or len(news_title) < 5:
                            continue
                        
                        # æª¢æŸ¥æ˜¯å¦èˆ‡ç›®æ¨™è‚¡ç¥¨ç›¸é—œ
                        if self._is_stock_related(news_title, stock_code, stock_name):
                            article_content = self._fetch_article_content(news_url, headers)
                            if article_content and len(article_content) > 50:
                                articles.append(NewsArticle(
                                    title=news_title,
                                    content=article_content,
                                    url=news_url,
                                    publish_time=self._extract_publish_time(soup, link),
                                    source='é‰…äº¨ç¶²'
                                ))
                                print(f"âœ“ å·²æ”¶é›†: {news_title}")
                                time.sleep(2)  # å¢åŠ å»¶é²é¿å…è¢«å°
                        
                    except Exception as e:
                        print(f"è™•ç†å–®æ¢æ–°èæ™‚å‡ºéŒ¯: {e}")
                        continue
                
                if len(articles) >= max_articles // 2:
                    break
                    
            except Exception as e:
                print(f"çˆ¬å–é‰…äº¨ç¶²æœå°‹é é¢å¤±æ•—: {e}")
                continue
        
        print(f"é‰…äº¨ç¶²å…±æ”¶é›†åˆ° {len(articles)} ç¯‡æ–°è")
        return articles
    
    def crawl_yahoo_news(self, stock_code: str, stock_name: str, max_articles: int = 10) -> List[NewsArticle]:
        """çˆ¬å–é›…è™è‚¡å¸‚æ–°è - å¢å¼·ç‰ˆ"""
        print(f"æ­£åœ¨çˆ¬å–Yahooè²¡ç¶“é—œæ–¼ {stock_code} {stock_name} çš„æ–°è...")
        articles = []
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
        }
        
        # å¤šå€‹Yahooæ–°èä¾†æº
        yahoo_urls = [
            f"https://tw.stock.yahoo.com/quote/{stock_code}.TW/news",
            f"https://tw.news.yahoo.com/è‚¡å¸‚/",
            "https://tw.stock.yahoo.com/news/category-tw-stock",
        ]
        
        for base_url in yahoo_urls:
            try:
                print(f"å˜—è©¦Yahoo URL: {base_url}")
                response = requests.get(base_url, headers=headers, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # å¤šç¨®Yahooæ–°èé¸æ“‡å™¨
                selectors = [
                    'h3[data-test-locator="cStreamItem"] a',
                    'div[data-testid*="news"] a',
                    '.news-stream-item a',
                    '.stream-item a', 
                    'h3 a[href*="news"]',
                    'a[href*="/news/"]',
                    '.title a',
                    '.newsTitle a'
                ]
                
                found_links = []
                for selector in selectors:
                    links = soup.select(selector)
                    if links:
                        found_links.extend(links)
                
                print(f"Yahooæ‰¾åˆ° {len(found_links)} å€‹æ–°èé€£çµ")
                
                for link in found_links:
                    if len(articles) >= max_articles:
                        break
                        
                    try:
                        news_url = link.get('href', '')
                        if not news_url:
                            continue
                            
                        if news_url.startswith('//'):
                            news_url = 'https:' + news_url
                        elif not news_url.startswith('http'):
                            news_url = 'https://tw.stock.yahoo.com' + news_url
                        
                        news_title = link.get_text(strip=True)
                        if not news_title or len(news_title) < 5:
                            continue
                        
                        # æª¢æŸ¥ç›¸é—œæ€§
                        if self._is_stock_related(news_title, stock_code, stock_name):
                            article_content = self._fetch_article_content(news_url, headers)
                            if article_content and len(article_content) > 50:
                                articles.append(NewsArticle(
                                    title=news_title,
                                    content=article_content,
                                    url=news_url,
                                    publish_time=self._extract_publish_time(soup, link),
                                    source='Yahooè²¡ç¶“'
                                ))
                                print(f"âœ“ Yahooå·²æ”¶é›†: {news_title}")
                                time.sleep(2)
                        
                    except Exception as e:
                        print(f"è™•ç†Yahooæ–°èæ™‚å‡ºéŒ¯: {e}")
                        continue
                
                if len(articles) >= max_articles // 2:
                    break
                    
            except Exception as e:
                print(f"çˆ¬å–Yahooæ–°èé é¢å¤±æ•—: {e}")
                continue
        
        print(f"Yahooè²¡ç¶“å…±æ”¶é›†åˆ° {len(articles)} ç¯‡æ–°è")
        return articles
    
    def crawl_udn_news(self, stock_code: str, stock_name: str, max_articles: int = 5) -> List[NewsArticle]:
        """çˆ¬å–è¯åˆæ–°èç¶²è‚¡å¸‚æ–°è"""
        print(f"æ­£åœ¨çˆ¬å–è¯åˆæ–°èç¶²é—œæ–¼ {stock_code} {stock_name} çš„æ–°è...")
        articles = []
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
        }
        
        try:
            # è¯åˆæ–°èç¶²è‚¡å¸‚ç‰ˆ
            udn_url = "https://udn.com/news/cate/2/6644"
            response = requests.get(udn_url, headers=headers, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°‹æ‰¾æ–°èé€£çµ
            news_links = soup.select('a[href*="/news/story/"]')
            
            for link in news_links:
                if len(articles) >= max_articles:
                    break
                    
                try:
                    news_url = link.get('href', '')
                    if not news_url.startswith('http'):
                        news_url = 'https://udn.com' + news_url
                    
                    news_title = link.get_text(strip=True)
                    if not news_title:
                        continue
                    
                    if self._is_stock_related(news_title, stock_code, stock_name):
                        article_content = self._fetch_article_content(news_url, headers)
                        if article_content and len(article_content) > 50:
                            articles.append(NewsArticle(
                                title=news_title,
                                content=article_content,
                                url=news_url,
                                publish_time=datetime.now().strftime('%Y-%m-%d %H:%M'),
                                source='è¯åˆæ–°èç¶²'
                            ))
                            print(f"âœ“ UDNå·²æ”¶é›†: {news_title}")
                            time.sleep(1)
                
                except Exception as e:
                    print(f"è™•ç†UDNæ–°èæ™‚å‡ºéŒ¯: {e}")
                    continue
        
        except Exception as e:
            print(f"çˆ¬å–è¯åˆæ–°èç¶²å¤±æ•—: {e}")
        
        print(f"è¯åˆæ–°èç¶²å…±æ”¶é›†åˆ° {len(articles)} ç¯‡æ–°è")
        return articles
    
    def crawl_chinatimes_news(self, stock_code: str, stock_name: str, max_articles: int = 5) -> List[NewsArticle]:
        """çˆ¬å–ä¸­æ™‚æ–°èç¶²è²¡ç¶“æ–°è"""
        print(f"æ­£åœ¨çˆ¬å–ä¸­æ™‚æ–°èç¶²é—œæ–¼ {stock_code} {stock_name} çš„æ–°è...")
        articles = []
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        
        try:
            # ä¸­æ™‚è²¡ç¶“æ–°è
            chinatimes_url = "https://www.chinatimes.com/money/?chdtv"
            response = requests.get(chinatimes_url, headers=headers, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°‹æ‰¾æ–°èé€£çµ
            news_links = soup.select('a[href*="/realtimenews/"]')
            
            for link in news_links:
                if len(articles) >= max_articles:
                    break
                    
                try:
                    news_url = link.get('href', '')
                    if not news_url.startswith('http'):
                        news_url = 'https://www.chinatimes.com' + news_url
                    
                    news_title = link.get_text(strip=True)
                    if not news_title:
                        continue
                    
                    if self._is_stock_related(news_title, stock_code, stock_name):
                        article_content = self._fetch_article_content(news_url, headers)
                        if article_content and len(article_content) > 50:
                            articles.append(NewsArticle(
                                title=news_title,
                                content=article_content,
                                url=news_url,
                                publish_time=datetime.now().strftime('%Y-%m-%d %H:%M'),
                                source='ä¸­æ™‚æ–°èç¶²'
                            ))
                            print(f"âœ“ ä¸­æ™‚å·²æ”¶é›†: {news_title}")
                            time.sleep(1)
                
                except Exception as e:
                    print(f"è™•ç†ä¸­æ™‚æ–°èæ™‚å‡ºéŒ¯: {e}")
                    continue
        
        except Exception as e:
            print(f"çˆ¬å–ä¸­æ™‚æ–°èç¶²å¤±æ•—: {e}")
        
        print(f"ä¸­æ™‚æ–°èç¶²å…±æ”¶é›†åˆ° {len(articles)} ç¯‡æ–°è")
        return articles
    
    def _is_stock_related(self, text: str, stock_code: str, stock_name: str) -> bool:
        """æ›´æ™ºèƒ½çš„è‚¡ç¥¨ç›¸é—œæ€§æª¢æŸ¥"""
        text_lower = text.lower()
        stock_code_lower = stock_code.lower()
        stock_name_lower = stock_name.lower()
        
        # ç›´æ¥åŒ¹é…
        direct_matches = [
            stock_code in text,
            stock_name in text,
            stock_code_lower in text_lower,
            stock_name_lower in text_lower,
            f"({stock_code})" in text,
            f"ï¼ˆ{stock_code}ï¼‰" in text,
            f" {stock_code} " in f" {text} ",
        ]
        
        if any(direct_matches):
            return True
        
        # æ¨¡ç³ŠåŒ¹é… - é‡å°å¸¸è¦‹çš„è‚¡ç¥¨ç°¡ç¨±
        stock_name_parts = stock_name_lower.replace('è‚¡ä»½æœ‰é™å…¬å¸', '').replace('å…¬å¸', '')
        for part in stock_name_parts.split():
            if len(part) >= 2 and part in text_lower:
                return True
        
        # è¡Œæ¥­é—œéµå­—åŒ¹é… (å¯ä»¥æ ¹æ“šä¸åŒè‚¡ç¥¨è‡ªå®šç¾©)
        industry_keywords = {
            '2330': ['åŠå°é«”', 'æ™¶åœ“', 'å°ç©é›»', 'tsmc', 'è£½ç¨‹'],
            '2317': ['é´»æµ·', 'å¯Œå£«åº·', 'iPhone', 'ä»£å·¥', 'çµ„è£'],
            '2454': ['è¯ç™¼ç§‘', 'æ™¶ç‰‡', 'IC', 'è™•ç†å™¨', 'mediatek'],
            # å¯ä»¥ç¹¼çºŒæ“´å……...
        }
        
        if stock_code in industry_keywords:
            for keyword in industry_keywords[stock_code]:
                if keyword in text_lower:
                    return True
        
        return False
    
    def _extract_publish_time(self, soup, link_element) -> str:
        """å˜—è©¦æå–ç™¼å¸ƒæ™‚é–“"""
        try:
            # å¸¸è¦‹çš„æ™‚é–“å…ƒç´ é¸æ“‡å™¨
            time_selectors = [
                'time',
                '.time',
                '.date',
                '.publish-time',
                '[datetime]',
                '.meta-info time'
            ]
            
            for selector in time_selectors:
                time_elem = soup.select_one(selector)
                if time_elem:
                    time_text = time_elem.get('datetime') or time_elem.get_text(strip=True)
                    if time_text:
                        return time_text
            
            return datetime.now().strftime('%Y-%m-%d %H:%M')
        except:
            return datetime.now().strftime('%Y-%m-%d %H:%M')
    
    def _fetch_article_content(self, url: str, headers: dict) -> str:
        """å¢å¼·ç‰ˆæ–°èå…§å®¹ç²å–"""
        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe', 'noscript']):
                tag.decompose()
            
            # ç§»é™¤å»£å‘Šå’Œç„¡é—œå…§å®¹
            for class_name in ['ad', 'advertisement', 'sidebar', 'related', 'comment', 'social', 'share']:
                for elem in soup.find_all(class_=re.compile(class_name, re.I)):
                    elem.decompose()
            
            # å¤šç¨®å…§å®¹é¸æ“‡å™¨ç­–ç•¥
            content_selectors = [
                # Yahooè²¡ç¶“
                'div.caas-body',
                'div[data-module="ArticleBody"]',
                '.article-wrap .article-body',
                
                # é‰…äº¨ç¶²
                '.news-content',
                '.article-body',
                '#newsText',
                
                # è¯åˆæ–°èç¶²
                'section.article-content__editor',
                '.article-content',
                
                # ä¸­æ™‚æ–°èç¶²
                '.article-body',
                '.article-content',
                
                # é€šç”¨é¸æ“‡å™¨
                'article',
                '.content',
                '.post-content',
                '.entry-content',
                'main',
                
                # æ®µè½é¸æ“‡å™¨ (æœ€å¾Œæ‰‹æ®µ)
                'p'
            ]
            
            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    # æå–æ–‡å­—ä¸¦æ¸…ç†
                    texts = []
                    for elem in elements:
                        text = elem.get_text(separator=' ', strip=True)
                        if text and len(text) > 20:  # éæ¿¾å¤ªçŸ­çš„å…§å®¹
                            texts.append(text)
                    
                    content = ' '.join(texts)
                    if len(content) > 100:  # ç¢ºä¿æœ‰è¶³å¤ å…§å®¹
                        break
            
            # å¦‚æœé‚„æ˜¯æ²’æœ‰å…§å®¹ï¼Œå˜—è©¦æå–æ‰€æœ‰æ®µè½
            if not content or len(content) < 100:
                paragraphs = soup.find_all('p')
                texts = []
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if text and len(text) > 10:
                        texts.append(text)
                content = ' '.join(texts)
            
            # æ¸…ç†å…§å®¹
            content = re.sub(r'\s+', ' ', content)  # åˆä½µå¤šé¤˜ç©ºç™½
            content = re.sub(r'[^\u4e00-\u9fff\w\s.,!?;:()ã€Œã€ã€ã€ã€ã€‘ã€ˆã€‰ã€Šã€‹]', '', content)  # ä¿ç•™ä¸­æ–‡å’ŒåŸºæœ¬æ¨™é»
            content = content.strip()
            
            # å»é™¤å¸¸è¦‹çš„ç„¡ç”¨å…§å®¹
            useless_patterns = [
                r'.*å»¶ä¼¸é–±è®€.*',
                r'.*ç›¸é—œæ–°è.*',
                r'.*æ›´å¤šæ–°è.*',
                r'.*æ¨è–¦é–±è®€.*',
                r'.*å…è²¬è²æ˜.*',
                r'.*ç‰ˆæ¬Šæ‰€æœ‰.*',
                r'.*æœ¬æ–‡ç”±.*æä¾›.*',
            ]
            
            for pattern in useless_patterns:
                content = re.sub(pattern, '', content, flags=re.IGNORECASE)
            
            return content.strip() if len(content) > 50 else ""
            
        except Exception as e:
            print(f"ç²å–æ–‡ç« å…§å®¹å¤±æ•— {url}: {e}")
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text(strip=True) for elem in elements])
                    if len(content) > 100:  # ç¢ºä¿æœ‰è¶³å¤ å…§å®¹
                        break
            
            # æ¸…ç†å…§å®¹
            content = re.sub(r'\s+', ' ', content)
            content = content.strip()
            
            return content if len(content) > 50 else ""
            
        except Exception as e:
            print(f"ç²å–æ–‡ç« å…§å®¹å¤±æ•— {url}: {e}")
            return ""
    
    def analyze_sentiment(self, article: NewsArticle) -> SentimentAnalysis:
        """åˆ†æå–®ç¯‡æ–‡ç« çš„æƒ…ç·’"""
        # åˆä½µæ¨™é¡Œå’Œå…§å®¹é€²è¡Œåˆ†æ
        full_text = f"{article.title} {article.content}"
        
        # ä½¿ç”¨jiebaé€²è¡Œä¸­æ–‡åˆ†è©
        words = list(jieba.cut(full_text))
        
        positive_score = 0
        negative_score = 0
        positive_words_found = []
        negative_words_found = []
        
        # è¨ˆç®—æƒ…ç·’åˆ†æ•¸
        for word in words:
            word = word.strip()
            if len(word) < 2:  # å¿½ç•¥å–®å­—å…ƒ
                continue
                
            # æ­£é¢è©å½™æª¢æŸ¥
            for pos_word, weight in self.positive_keywords.items():
                if pos_word in word or word in pos_word:
                    positive_score += weight
                    if pos_word not in positive_words_found:
                        positive_words_found.append(pos_word)
            
            # è² é¢è©å½™æª¢æŸ¥
            for neg_word, weight in self.negative_keywords.items():
                if neg_word in word or word in neg_word:
                    negative_score += weight
                    if neg_word not in negative_words_found:
                        negative_words_found.append(neg_word)
        
        # è¨ˆç®—æœ€çµ‚æƒ…ç·’
        total_score = positive_score + negative_score
        if total_score == 0:
            sentiment = 'NEUTRAL'
            confidence = 30
        else:
            net_score = positive_score - negative_score
            if net_score > 2:
                sentiment = 'POSITIVE'
                confidence = min(70 + (net_score * 5), 95)
            elif net_score < -2:
                sentiment = 'NEGATIVE'  
                confidence = min(70 + (abs(net_score) * 5), 95)
            else:
                sentiment = 'NEUTRAL'
                confidence = 40
        
        # ç”Ÿæˆæ‘˜è¦
        sentiment_text = {'POSITIVE': 'æ­£é¢', 'NEGATIVE': 'è² é¢', 'NEUTRAL': 'ä¸­æ€§'}
        summary = f"æƒ…ç·’å‚¾å‘ï¼š{sentiment_text[sentiment]} (ä¿¡å¿ƒåº¦: {confidence}%)"
        
        if positive_words_found:
            summary += f"ï¼Œæ­£é¢é—œéµå­—ï¼š{', '.join(positive_words_found[:5])}"
        if negative_words_found:
            summary += f"ï¼Œè² é¢é—œéµå­—ï¼š{', '.join(negative_words_found[:5])}"
        
        return SentimentAnalysis(
            sentiment=sentiment,
            confidence=confidence,
            positive_score=positive_score,
            negative_score=negative_score,
            key_positive_words=positive_words_found[:10],
            key_negative_words=negative_words_found[:10],
            summary=summary
        )
    
    def analyze_stock_news(self, stock_code: str, stock_name: str, max_articles: int = 10) -> Dict:
        """åˆ†ææŒ‡å®šè‚¡ç¥¨çš„æ–°èæƒ…ç·’ - å¢å¼·ç‰ˆå¤šä¾†æºçˆ¬å–"""
        print(f"é–‹å§‹åˆ†æè‚¡ç¥¨ {stock_code} {stock_name} çš„æ–°èæƒ…ç·’...")
        print("=" * 50)
        
        all_articles = []
        
        # ä¸¦è¡Œçˆ¬å–å¤šå€‹æ–°èæºï¼Œç¢ºä¿èƒ½ç²å¾—è¶³å¤ çš„æ–°è
        news_sources = [
            ('Yahooè²¡ç¶“', lambda: self.crawl_yahoo_news(stock_code, stock_name, max_articles//3)),
            ('é‰…äº¨ç¶²', lambda: self.crawl_cnyes_news(stock_code, stock_name, max_articles//3)),
            ('è¯åˆæ–°èç¶²', lambda: self.crawl_udn_news(stock_code, stock_name, max_articles//4)),
            ('ä¸­æ™‚æ–°èç¶²', lambda: self.crawl_chinatimes_news(stock_code, stock_name, max_articles//4)),
        ]
        
        for source_name, crawl_func in news_sources:
            try:
                print(f"\nğŸ” æ­£åœ¨çˆ¬å– {source_name}...")
                articles = crawl_func()
                if articles:
                    all_articles.extend(articles)
                    print(f"âœ… {source_name} æˆåŠŸç²å– {len(articles)} ç¯‡æ–°è")
                else:
                    print(f"âš ï¸ {source_name} æœªç²å–åˆ°ç›¸é—œæ–°è")
                
                # å¦‚æœå·²ç¶“æ”¶é›†åˆ°è¶³å¤ çš„æ–°èï¼Œå¯ä»¥æå‰çµæŸ
                if len(all_articles) >= max_articles:
                    print(f"âœ… å·²æ”¶é›†åˆ°è¶³å¤ çš„æ–°è ({len(all_articles)} ç¯‡)ï¼Œåœæ­¢çˆ¬å–")
                    break
                    
            except Exception as e:
                print(f"âŒ {source_name} çˆ¬å–å¤±æ•—: {e}")
                continue
        
        # å»é‡è™•ç† (åŸºæ–¼æ¨™é¡Œç›¸ä¼¼æ€§)
        unique_articles = self._remove_duplicate_articles(all_articles)
        
        print(f"\nğŸ“Š çˆ¬å–ç¸½çµ:")
        print(f"   åŸå§‹æ”¶é›†: {len(all_articles)} ç¯‡")
        print(f"   å»é‡å¾Œ: {len(unique_articles)} ç¯‡")
        
        if not unique_articles:
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–°èï¼Œå˜—è©¦æ›´å¯¬é¬†çš„æœå°‹
            print(f"\nğŸ”„ æœªæ‰¾åˆ°ç›´æ¥ç›¸é—œæ–°èï¼Œå˜—è©¦è¡Œæ¥­æ–°è...")
            industry_articles = self._crawl_industry_news(stock_code, stock_name)
            unique_articles.extend(industry_articles)
        
        if not unique_articles:
            return {
                'success': False,
                'message': f'æœªèƒ½ç²å–åˆ° {stock_code} {stock_name} çš„ç›¸é—œæ–°èã€‚å¯èƒ½åŸå› ï¼š1.è©²è‚¡ç¥¨è¿‘æœŸæ–°èè¼ƒå°‘ 2.ç¶²ç«™åçˆ¬èŸ²é™åˆ¶ 3.è‚¡ç¥¨ä»£ç¢¼æˆ–åç¨±éœ€è¦èª¿æ•´',
                'sentiment_summary': None,
                'articles': []
            }
        
        # é™åˆ¶åˆ†æçš„æ–°èæ•¸é‡
        articles_to_analyze = unique_articles[:max_articles]
        
        print(f"\nğŸ¯ é–‹å§‹åˆ†æ {len(articles_to_analyze)} ç¯‡æ–°èçš„æƒ…ç·’...")
        print("-" * 50)
        
        # åˆ†ææ¯ç¯‡æ–‡ç« 
        analyzed_articles = []
        total_positive = 0
        total_negative = 0
        all_positive_words = []
        all_negative_words = []
        
        for i, article in enumerate(articles_to_analyze, 1):
            print(f"åˆ†æç¬¬ {i} ç¯‡: {article.title[:50]}...")
            
            sentiment_result = self.analyze_sentiment(article)
            analyzed_articles.append({
                'article': article,
                'sentiment': sentiment_result
            })
            
            total_positive += sentiment_result.positive_score
            total_negative += sentiment_result.negative_score
            all_positive_words.extend(sentiment_result.key_positive_words)
            all_negative_words.extend(sentiment_result.key_negative_words)
            
            # é¡¯ç¤ºæ¯ç¯‡æ–‡ç« çš„ç°¡è¦çµæœ
            sentiment_icon = {'POSITIVE': 'ğŸ“ˆ', 'NEGATIVE': 'ğŸ“‰', 'NEUTRAL': 'ğŸ“Š'}[sentiment_result.sentiment]
            print(f"   çµæœ: {sentiment_icon} {sentiment_result.sentiment} ({sentiment_result.confidence}%)")
        
        # è¨ˆç®—æ•´é«”æƒ…ç·’
        if total_positive + total_negative == 0:
            overall_sentiment = 'NEUTRAL'
            overall_confidence = 30
        else:
            net_sentiment = total_positive - total_negative
            positive_ratio = total_positive / (total_positive + total_negative)
            
            if net_sentiment > 5:
                overall_sentiment = 'POSITIVE'
                overall_confidence = min(60 + (net_sentiment * 2), 90)
            elif net_sentiment < -5:
                overall_sentiment = 'NEGATIVE'
                overall_confidence = min(60 + (abs(net_sentiment) * 2), 90)
            else:
                overall_sentiment = 'NEUTRAL'
                overall_confidence = max(40, min(60, 50 + abs(net_sentiment) * 2))
        
        # çµ±è¨ˆé—œéµå­—
        positive_word_counts = Counter(all_positive_words)
        negative_word_counts = Counter(all_negative_words)
        
        sentiment_summary = {
            'overall_sentiment': overall_sentiment,
            'confidence': overall_confidence,
            'total_articles': len(analyzed_articles),
            'positive_articles': len([a for a in analyzed_articles if a['sentiment'].sentiment == 'POSITIVE']),
            'negative_articles': len([a for a in analyzed_articles if a['sentiment'].sentiment == 'NEGATIVE']),
            'neutral_articles': len([a for a in analyzed_articles if a['sentiment'].sentiment == 'NEUTRAL']),
            'total_positive_score': total_positive,
            'total_negative_score': total_negative,
            'net_sentiment_score': total_positive - total_negative,
            'top_positive_keywords': positive_word_counts.most_common(10),
            'top_negative_keywords': negative_word_counts.most_common(10),
            'news_sources': list(set([article.source for article in articles_to_analyze]))
        }
        
        print(f"\nâœ… æƒ…ç·’åˆ†æå®Œæˆï¼")
        print(f"   æ•´é«”æƒ…ç·’: {overall_sentiment} (ä¿¡å¿ƒåº¦: {overall_confidence}%)")
        print(f"   æ­£é¢åˆ†æ•¸: {total_positive}, è² é¢åˆ†æ•¸: {total_negative}")
        
        return {
            'success': True,
            'sentiment_summary': sentiment_summary,
            'articles': analyzed_articles
        }
    
    def _remove_duplicate_articles(self, articles: List[NewsArticle]) -> List[NewsArticle]:
        """å»é™¤é‡è¤‡çš„æ–°èæ–‡ç« """
        unique_articles = []
        seen_titles = set()
        
        for article in articles:
            # ç°¡åŒ–æ¨™é¡Œç”¨æ–¼æ¯”è¼ƒ
            simplified_title = re.sub(r'[^\u4e00-\u9fff\w]', '', article.title.lower())
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¤‡
            is_duplicate = False
            for seen_title in seen_titles:
                # å¦‚æœå…©å€‹æ¨™é¡Œæœ‰80%ä»¥ä¸Šçš„ç›¸ä¼¼æ€§ï¼Œè¦–ç‚ºé‡è¤‡
                if self._text_similarity(simplified_title, seen_title) > 0.8:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_articles.append(article)
                seen_titles.add(simplified_title)
        
        return unique_articles
    
    def _text_similarity(self, text1: str, text2: str) -> float:
        """è¨ˆç®—å…©å€‹æ–‡æœ¬çš„ç›¸ä¼¼æ€§"""
        if not text1 or not text2:
            return 0.0
        
        # ç°¡å–®çš„å­—ç¬¦é›†åˆç›¸ä¼¼æ€§è¨ˆç®—
        set1 = set(text1)
        set2 = set(text2)
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        return intersection / union if union > 0 else 0.0
    
    def _crawl_industry_news(self, stock_code: str, stock_name: str) -> List[NewsArticle]:
        """ç•¶æ‰¾ä¸åˆ°ç›´æ¥ç›¸é—œæ–°èæ™‚ï¼Œæœå°‹è¡Œæ¥­ç›¸é—œæ–°è"""
        print("ğŸ” æœå°‹è¡Œæ¥­ç›¸é—œæ–°è...")
        
        # æ ¹æ“šè‚¡ç¥¨ä»£ç¢¼ç¢ºå®šè¡Œæ¥­é—œéµå­—
        industry_map = {
            '2330': ['åŠå°é«”', 'æ™¶åœ“', 'å°ç©é›»'],
            '2317': ['é›»å­ä»£å·¥', 'é´»æµ·', 'çµ„è£'],
            '2454': ['ICè¨­è¨ˆ', 'è¯ç™¼ç§‘', 'æ™¶ç‰‡'],
            '2881': ['é‡‘è', 'éŠ€è¡Œ', 'å¯Œé‚¦'],
            '2891': ['é‡‘è', 'éŠ€è¡Œ', 'ä¸­ä¿¡'],
            # å¯ä»¥ç¹¼çºŒæ“´å……...
        }
        
        industry_keywords = industry_map.get(stock_code, [stock_name])
        articles = []
        
        # é€™è£¡å¯ä»¥å¯¦ä½œæ›´å¯¬é¬†çš„è¡Œæ¥­æ–°èæœå°‹
        # ç‚ºäº†ç°¡åŒ–ï¼Œé€™è£¡å…ˆè¿”å›ç©ºåˆ—è¡¨
        
        return articles

def format_sentiment_report(analysis_result: Dict, stock_code: str, stock_name: str) -> str:
    """æ ¼å¼åŒ–æƒ…ç·’åˆ†æå ±å‘Š"""
    if not analysis_result['success']:
        return f"âŒ {stock_code} {stock_name} æ–°èæƒ…ç·’åˆ†æå¤±æ•—ï¼š{analysis_result['message']}"
    
    summary = analysis_result['sentiment_summary']
    articles = analysis_result['articles']
    
    # æƒ…ç·’emojiå°æ‡‰
    sentiment_emoji = {
        'POSITIVE': 'ğŸ“ˆ',
        'NEGATIVE': 'ğŸ“‰', 
        'NEUTRAL': 'ğŸ“Š'
    }
    
    sentiment_text = {
        'POSITIVE': 'æ­£é¢',
        'NEGATIVE': 'è² é¢',
        'NEUTRAL': 'ä¸­æ€§'
    }
    
    output = []
    output.append("=" * 60)
    output.append(f"ğŸ“° æ–°èæƒ…ç·’åˆ†æå ±å‘Š - {stock_code} {stock_name}")
    output.append("=" * 60)
    output.append("")
    
    # æ•´é«”æƒ…ç·’
    overall = summary['overall_sentiment']
    output.append(f"{sentiment_emoji[overall]} æ•´é«”æ–°èæƒ…ç·’: {sentiment_text[overall]}")
    output.append(f"ğŸ¯ ä¿¡å¿ƒç¨‹åº¦: {summary['confidence']}%")
    output.append("")
    
    # çµ±è¨ˆè³‡è¨Š
    output.append("ğŸ“Š æ–°èçµ±è¨ˆ:")
    output.append(f"   ç¸½æ–°èæ•¸: {summary['total_articles']} ç¯‡")
    output.append(f"   æ­£é¢æ–°è: {summary['positive_articles']} ç¯‡")
    output.append(f"   è² é¢æ–°è: {summary['negative_articles']} ç¯‡")
    output.append(f"   ä¸­æ€§æ–°è: {summary['neutral_articles']} ç¯‡")
    output.append("")
    
    # æƒ…ç·’åˆ†æ•¸
    output.append(f"ğŸ“ˆ æ­£é¢æƒ…ç·’åˆ†æ•¸: {summary['total_positive_score']}")
    output.append(f"ğŸ“‰ è² é¢æƒ…ç·’åˆ†æ•¸: {summary['total_negative_score']}")
    net_score = summary['total_positive_score'] - summary['total_negative_score']
    output.append(f"âš–ï¸  æ·¨æƒ…ç·’åˆ†æ•¸: {net_score:+d}")
    output.append("")
    
    # é—œéµå­—åˆ†æ
    if summary['top_positive_keywords']:
        output.append("ğŸ”‘ æ­£é¢é—œéµå­—:")
        for word, count in summary['top_positive_keywords'][:8]:
            output.append(f"   â€¢ {word} (å‡ºç¾{count}æ¬¡)")
    
    if summary['top_negative_keywords']:
        output.append("")
        output.append("âš ï¸  è² é¢é—œéµå­—:")
        for word, count in summary['top_negative_keywords'][:8]:
            output.append(f"   â€¢ {word} (å‡ºç¾{count}æ¬¡)")
    
    output.append("")
    output.append("ğŸ“‹ å„ç¯‡æ–°èåˆ†æ:")
    output.append("-" * 40)
    
    # é¡¯ç¤ºæ¯ç¯‡æ–°èçš„ç°¡è¦åˆ†æ
    for i, item in enumerate(articles[:5], 1):  # åªé¡¯ç¤ºå‰5ç¯‡è©³ç´°è³‡è¨Š
        article = item['article']
        sentiment = item['sentiment']
        
        output.append(f"{i}. {sentiment_emoji[sentiment.sentiment]} {article.title}")
        output.append(f"   ä¾†æº: {article.source}")
        output.append(f"   æƒ…ç·’: {sentiment_text[sentiment.sentiment]} ({sentiment.confidence}%)")
        if sentiment.key_positive_words:
            output.append(f"   æ­£é¢è©: {', '.join(sentiment.key_positive_words[:3])}")
        if sentiment.key_negative_words:
            output.append(f"   è² é¢è©: {', '.join(sentiment.key_negative_words[:3])}")
        output.append("")
    
    if len(articles) > 5:
        output.append(f"... é‚„æœ‰ {len(articles) - 5} ç¯‡æ–°èæœªè©³ç´°é¡¯ç¤º")
    
    output.append("")
    output.append("ğŸ’¡ æŠ•è³‡åƒè€ƒå»ºè­°:")
    
    if overall == 'POSITIVE':
        if summary['confidence'] > 70:
            output.append("   â€¢ æ–°èé¢æ•´é«”åå‘æ­£é¢ï¼Œå¯èƒ½å°è‚¡åƒ¹æœ‰æ­£é¢åŠ©ç›Š")
            output.append("   â€¢ ä½†ä»éœ€æ­é…æŠ€è¡“é¢èˆ‡åŸºæœ¬é¢åˆ†æ")
        else:
            output.append("   â€¢ æ–°èé¢ç•¥åæ­£é¢ï¼Œä½†è¨Šè™Ÿä¸å¤ å¼·çƒˆ")
    elif overall == 'NEGATIVE':
        if summary['confidence'] > 70:
            output.append("   â€¢ æ–°èé¢æ•´é«”åå‘è² é¢ï¼Œå¯èƒ½å°è‚¡åƒ¹é€ æˆå£“åŠ›")
            output.append("   â€¢ å»ºè­°è¬¹æ…è©•ä¼°æŠ•è³‡é¢¨éšª")
        else:
            output.append("   â€¢ æ–°èé¢ç•¥åè² é¢ï¼Œéœ€æŒçºŒè§€å¯Ÿ")
    else:
        output.append("   â€¢ æ–°èé¢æ•´é«”ä¸­æ€§ï¼Œå°è‚¡åƒ¹å½±éŸ¿æœ‰é™")
        output.append("   â€¢ å»ºè­°ä»¥æŠ€è¡“é¢åˆ†æç‚ºä¸»è¦åƒè€ƒ")
    
    output.append("")
    output.append("âš ï¸  é‡è¦æé†’:")
    output.append("   â€¢ æ–°èæƒ…ç·’åˆ†æåƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæŠ•è³‡å»ºè­°")
    output.append("   â€¢ è«‹æ­é…æŠ€è¡“åˆ†æèˆ‡åŸºæœ¬é¢åˆ†æç¶œåˆåˆ¤æ–·")
    output.append("   â€¢ å¸‚å ´æƒ…ç·’è®ŠåŒ–å¿«é€Ÿï¼Œéœ€æŒçºŒè¿½è¹¤æœ€æ–°æ¶ˆæ¯")
    output.append("   â€¢ å»ºè­°å°‡æ­¤åˆ†æçµæœèˆ‡æŠ€è¡“è©•åˆ†ä¸€ä½µè€ƒé‡")
    
    return "\n".join(output)

# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # æ¸¬è©¦ç”¨çš„è‚¡ç¥¨æ¸…å–®
    STOCK_MAP = {
        "2330": "å°ç©é›»", "2317": "é´»æµ·", "2454": "è¯ç™¼ç§‘", "2308": "å°é”é›»",
        "2382": "å»£é”", "2881": "å¯Œé‚¦é‡‘", "2891": "ä¸­ä¿¡é‡‘", "2882": "åœ‹æ³°é‡‘",
        "2357": "è¯ç¢©", "3231": "ç·¯å‰µ", "2412": "ä¸­è¯é›»", "1301": "å°å¡‘",
        "1303": "å—äº", "2002": "ä¸­é‹¼", "2886": "å…†è±é‡‘", "2603": "é•·æ¦®"
    }
    
    # å‰µå»ºåˆ†æå™¨å¯¦ä¾‹
    analyzer = NewsEmotionAnalyzer()
    
    # æ¸¬è©¦è‚¡ç¥¨ - å¯ä»¥ä¿®æ”¹é€™è£¡
    test_stock_code = "2330"
    test_stock_name = STOCK_MAP[test_stock_code]
    
    print(f"é–‹å§‹åˆ†æ {test_stock_code} {test_stock_name} çš„æ–°èæƒ…ç·’...")
    
    # åŸ·è¡Œåˆ†æ
    result = analyzer.analyze_stock_news(test_stock_code, test_stock_name, max_articles=10)
    
    # è¼¸å‡ºæ ¼å¼åŒ–å ±å‘Š
    report = format_sentiment_report(result, test_stock_code, test_stock_name)
    print(report)